import os
import re
import asyncio
from pathlib import Path
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableLambda
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_community.chat_message_histories import FileChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langserve import add_routes

# --- 1. MCP å·¥å…·è¿œç¨‹åŠ è½½ï¼ˆåŒ…æ‹¬ history å·¥å…·ï¼‰ ---
MCP_SERVERS = {
    "knowledge": {
        "url": "http://localhost:9000/mcp/",
        "transport": "streamable_http",
    },
    "history": {
        "url": "http://localhost:9100/mcp/",
        "transport": "streamable_http",
    },
}


async def load_tools():
    client = MultiServerMCPClient(MCP_SERVERS)
    return await client.get_tools()


mcp_tools = asyncio.run(load_tools())


# å•ç‹¬è·å–å†å²ç›¸å…³å·¥å…·
def find_tool(name):
    for t in mcp_tools:
        if t.name == name:
            return t
    raise RuntimeError(f"Tool {name} not found!")


search_history_tool = find_tool("search_history")
add_history_tool = find_tool("add_history")
search_knowledge_tool = find_tool("search_multimodal_knowledge")  # å¯é€‰

# --- 2. LLM & Agent ---
model = ChatOpenAI(
    model="qwen-plus",
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    temperature=0.7,
)

agent_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ä½ æ˜¯åŸç¥AIåŠ©æ‰‹ã€‚"
            "\nå’Œç”¨æˆ·å†å²å¯¹è¯ä¸­ä¸æœ¬é—®é¢˜æœ€ç›¸å…³çš„å†…å®¹å¦‚ä¸‹ï¼š\n{history_context}\n"
            "æ¸¸æˆçŸ¥è¯†åº“çš„ç›¸å…³å†…å®¹å¦‚ä¸‹ï¼š\n{knowledge_context}\n"
            "è¯·æ ¹æ®è¿™äº›ä¿¡æ¯å›ç­”ç”¨æˆ·ã€‚",
        ),
        MessagesPlaceholder(variable_name="history"),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ]
)


async def agent_chain_fn(inputs: dict) -> str:
    session_id = inputs.get("session_id", "default")
    query = inputs["input"]

    # 1. ç”¨history MCPå·¥å…·å¬å›ç›¸å…³å†å²
    history_context = await search_history_tool.ainvoke(
        {"session_id": session_id, "query": query, "k": 3}
    )
    # 2. ç”¨knowledge MCPå·¥å…·å¬å›çŸ¥è¯†ï¼ˆå¯é€‰ï¼Œä¹Ÿå¯ä»¥è®©agentè‡ªä¸»è°ƒå·¥å…·ï¼‰
    knowledge_context = await search_knowledge_tool.ainvoke({"query": query})
    # 3. ç»„è£…prompt
    payload = {
        "input": query,
        "session_id": session_id,
        "history_context": history_context,
        "knowledge_context": knowledge_context,
        "history": inputs.get("history", []),
    }
    # 4. Agent function_calling
    agent = create_tool_calling_agent(llm=model, tools=mcp_tools, prompt=agent_prompt)
    executor = AgentExecutor(
        agent=agent,
        tools=mcp_tools,
        verbose=True,
        handle_parsing_errors=True,
        max_iterations=3,
    )
    result = await executor.ainvoke(payload)
    # 5. ä¿å­˜æœ¬è½®å†å²ï¼ˆç”¨æˆ·é—®é¢˜ã€AIå›ç­”éƒ½å­˜åˆ°å†å²RAGå·¥å…·é‡Œï¼‰
    await add_history_tool.ainvoke(
        {"session_id": session_id, "role": "user", "message": query}
    )
    await add_history_tool.ainvoke(
        {"session_id": session_id, "role": "ai", "message": result["output"]}
    )
    return result["output"]


class InputChat(BaseModel):
    input: str
    session_id: str = "default"


agent_chain = RunnableLambda(agent_chain_fn).with_types(input_type=InputChat)


# --- 3. session historyï¼ˆä¼ ç»Ÿæ¶ˆæ¯å†å², å¯é€‰, æ”¯æŒplaygroundï¼‰ ---
def _valid_id(s: str) -> bool:
    return bool(re.fullmatch(r"[a-zA-Z0-9\-_]+", s))


def make_history_factory(base: str):
    p = Path(base)
    p.mkdir(parents=True, exist_ok=True)

    def _factory(session_id: str) -> FileChatMessageHistory:
        if not _valid_id(session_id):
            raise HTTPException(400, "Session ID åªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—ã€'-'ã€'_'")
        return FileChatMessageHistory(str(p / f"{session_id}.json"))

    return _factory


# with_history_chain = RunnableWithMessageHistory(
#     agent_chain,
#     make_history_factory("chat_histories"),
#     input_messages_key="input",
#     history_messages_key="history",
# ).with_types(input_type=InputChat)

# --- 4. FastAPI/Serve ---
app = FastAPI(
    title="Genshin Assistant (å†å²RAG MCP)",
    version="v0.3-mcp-history",
    description="åŸç¥ AI åŠ©æ‰‹ï¼ŒRAGçŸ¥è¯†åº“+å†å²å‘é‡è®°å¿†(MCPå·¥å…·)",
)
add_routes(app, agent_chain, path="/chat")


@app.get("/")
async def root():
    return {
        "how_to": "POST /chat?session_id=your_id  JSON: {'input': 'ä½ çš„é—®é¢˜'}",
        "tip": "æœ¬æœåŠ¡æ”¯æŒRAGçŸ¥è¯†åº“ä¸å†å²RAGï¼ˆå†å²é€šè¿‡MCPå·¥å…·è¿œç¨‹ç®¡ç†ï¼‰",
    }


if __name__ == "__main__":
    import uvicorn

    print("ğŸš€ http://localhost:8000")
    uvicorn.run(app, host="localhost", port=8000)
